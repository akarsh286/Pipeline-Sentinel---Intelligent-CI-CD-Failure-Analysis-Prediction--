[
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "jsonify",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "joblib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "joblib",
        "description": "joblib",
        "detail": "joblib",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "RandomizedSearchCV",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "LogisticRegression",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "LogisticRegression",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "RandomForestClassifier",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "XGBClassifier",
        "importPath": "xgboost",
        "description": "xgboost",
        "isExtraImport": true,
        "detail": "xgboost",
        "documentation": {}
    },
    {
        "label": "classification_report",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "make_scorer",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "classification_report",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "SMOTE",
        "importPath": "imblearn.over_sampling",
        "description": "imblearn.over_sampling",
        "isExtraImport": true,
        "detail": "imblearn.over_sampling",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "predict",
        "kind": 2,
        "importPath": "prediction-service.app",
        "description": "prediction-service.app",
        "peekOfCode": "def predict():\n    if not model or not feature_columns:\n        return jsonify({'error': 'Model is not loaded. Please train the model first.'}), 500\n    try:\n        data = request.get_json()\n        live_df = pd.DataFrame([data])\n        # Perform the same feature engineering\n        live_df['change_size'] = live_df['lines_added'] + live_df['lines_deleted']\n        live_df['add_delete_ratio'] = live_df['lines_added'] / (live_df['lines_deleted'] + 1)\n        # Align columns with the training data",
        "detail": "prediction-service.app",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "prediction-service.app",
        "description": "prediction-service.app",
        "peekOfCode": "app = Flask(__name__)\n# ================== LOAD THE TRAINED MODEL AND THRESHOLD ==================\ntry:\n    model = joblib.load('risk_model.pkl')\n    print(\"✅ Successfully loaded trained model: 'risk_model.pkl'\")\n    feature_columns = joblib.load('feature_columns.pkl')\n    print(\"✅ Successfully loaded feature columns: 'feature_columns.pkl'\")\n    # Load our new optimal threshold\n    optimal_threshold = joblib.load('optimal_threshold.pkl')\n    print(f\"✅ Successfully loaded optimal threshold: {optimal_threshold:.4f}\")",
        "detail": "prediction-service.app",
        "documentation": {}
    },
    {
        "label": "get_rate_limit",
        "kind": 2,
        "importPath": "prediction-service.collect_data",
        "description": "prediction-service.collect_data",
        "peekOfCode": "def get_rate_limit():\n    \"\"\"Checks the current GitHub API rate limit status.\"\"\"\n    response = requests.get('https://api.github.com/rate_limit', headers=HEADERS)\n    if response.status_code == 200:\n        rate_limit_data = response.json()['resources']['core']\n        print(f\"Rate Limit: {rate_limit_data['remaining']}/{rate_limit_data['limit']} requests remaining.\")\n        return rate_limit_data['remaining']\n    return 0\ndef get_build_status_for_commit(sha):\n    \"\"\"",
        "detail": "prediction-service.collect_data",
        "documentation": {}
    },
    {
        "label": "get_build_status_for_commit",
        "kind": 2,
        "importPath": "prediction-service.collect_data",
        "description": "prediction-service.collect_data",
        "peekOfCode": "def get_build_status_for_commit(sha):\n    \"\"\"\n    For a given commit SHA, find the conclusion of the main CI check run.\n    This is the most crucial part: linking a PR to its build outcome.\n    \"\"\"\n    url = f'https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/commits/{sha}/check-runs'\n    response = requests.get(url, headers=HEADERS)\n    if response.status_code == 200:\n        check_runs = response.json().get('check_runs', [])\n        # We look for a common CI workflow name. This might need to be adjusted for other repos.",
        "detail": "prediction-service.collect_data",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "prediction-service.collect_data",
        "description": "prediction-service.collect_data",
        "peekOfCode": "def main():\n    \"\"\"Main function to collect data and save it to a CSV.\"\"\"\n    all_pr_data = []\n    print(\"Starting large-scale data collection...\")\n    get_rate_limit()\n    for page in range(1, PAGES_TO_FETCH + 1):\n        print(f\"\\nFetching page {page}/{PAGES_TO_FETCH} of pull requests...\")\n        # Fetch closed pull requests (as they have a final build status)\n        prs_url = f'https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/pulls?state=closed&per_page=100&page={page}'\n        response = requests.get(prs_url, headers=HEADERS)",
        "detail": "prediction-service.collect_data",
        "documentation": {}
    },
    {
        "label": "GITHUB_TOKEN",
        "kind": 5,
        "importPath": "prediction-service.collect_data",
        "description": "prediction-service.collect_data",
        "peekOfCode": "GITHUB_TOKEN = os.getenv('GITHUB_PERSONAL_ACCESS_TOKEN')\nif not GITHUB_TOKEN:\n    raise ValueError(\"GitHub token not found. Please create a .env file with GITHUB_PERSONAL_ACCESS_TOKEN.\")\n# The repository we will collect data from. A large, active repo is best.\nREPO_OWNER = 'pandas-dev'\nREPO_NAME = 'pandas'\n# --- THE ONLY CHANGE IS HERE ---\n# We are now fetching a much larger dataset for our real model.\n# This will take a long time to run.\nPAGES_TO_FETCH = 35 # Changed from 3 to 25",
        "detail": "prediction-service.collect_data",
        "documentation": {}
    },
    {
        "label": "REPO_OWNER",
        "kind": 5,
        "importPath": "prediction-service.collect_data",
        "description": "prediction-service.collect_data",
        "peekOfCode": "REPO_OWNER = 'pandas-dev'\nREPO_NAME = 'pandas'\n# --- THE ONLY CHANGE IS HERE ---\n# We are now fetching a much larger dataset for our real model.\n# This will take a long time to run.\nPAGES_TO_FETCH = 35 # Changed from 3 to 25\nHEADERS = {\n    'Authorization': f'token {GITHUB_TOKEN}',\n    'Accept': 'application/vnd.github.v3+json'\n}",
        "detail": "prediction-service.collect_data",
        "documentation": {}
    },
    {
        "label": "REPO_NAME",
        "kind": 5,
        "importPath": "prediction-service.collect_data",
        "description": "prediction-service.collect_data",
        "peekOfCode": "REPO_NAME = 'pandas'\n# --- THE ONLY CHANGE IS HERE ---\n# We are now fetching a much larger dataset for our real model.\n# This will take a long time to run.\nPAGES_TO_FETCH = 35 # Changed from 3 to 25\nHEADERS = {\n    'Authorization': f'token {GITHUB_TOKEN}',\n    'Accept': 'application/vnd.github.v3+json'\n}\n# --- SCRIPT LOGIC ---",
        "detail": "prediction-service.collect_data",
        "documentation": {}
    },
    {
        "label": "PAGES_TO_FETCH",
        "kind": 5,
        "importPath": "prediction-service.collect_data",
        "description": "prediction-service.collect_data",
        "peekOfCode": "PAGES_TO_FETCH = 35 # Changed from 3 to 25\nHEADERS = {\n    'Authorization': f'token {GITHUB_TOKEN}',\n    'Accept': 'application/vnd.github.v3+json'\n}\n# --- SCRIPT LOGIC ---\ndef get_rate_limit():\n    \"\"\"Checks the current GitHub API rate limit status.\"\"\"\n    response = requests.get('https://api.github.com/rate_limit', headers=HEADERS)\n    if response.status_code == 200:",
        "detail": "prediction-service.collect_data",
        "documentation": {}
    },
    {
        "label": "HEADERS",
        "kind": 5,
        "importPath": "prediction-service.collect_data",
        "description": "prediction-service.collect_data",
        "peekOfCode": "HEADERS = {\n    'Authorization': f'token {GITHUB_TOKEN}',\n    'Accept': 'application/vnd.github.v3+json'\n}\n# --- SCRIPT LOGIC ---\ndef get_rate_limit():\n    \"\"\"Checks the current GitHub API rate limit status.\"\"\"\n    response = requests.get('https://api.github.com/rate_limit', headers=HEADERS)\n    if response.status_code == 200:\n        rate_limit_data = response.json()['resources']['core']",
        "detail": "prediction-service.collect_data",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "df = pd.read_csv('training_data_large.csv')\nstatus_counts = df['build_status'].value_counts()\ndf = pd.get_dummies(df, columns=['author_association'], prefix='author')\ndf.fillna(0, inplace=True)\ndf['change_size'] = df['lines_added'] + df['lines_deleted']\ndf['add_delete_ratio'] = df['lines_added'] / (df['lines_deleted'] + 1)\nprint(\"Data preparation complete.\")\nprint(\"-\" * 40)\n# --- 4. Prepare Data and Split ---\nprint(\"\\n--- Step 4: Preparing and Splitting Data ---\")",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "status_counts",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "status_counts = df['build_status'].value_counts()\ndf = pd.get_dummies(df, columns=['author_association'], prefix='author')\ndf.fillna(0, inplace=True)\ndf['change_size'] = df['lines_added'] + df['lines_deleted']\ndf['add_delete_ratio'] = df['lines_added'] / (df['lines_deleted'] + 1)\nprint(\"Data preparation complete.\")\nprint(\"-\" * 40)\n# --- 4. Prepare Data and Split ---\nprint(\"\\n--- Step 4: Preparing and Splitting Data ---\")\ny = df['build_status']",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "df = pd.get_dummies(df, columns=['author_association'], prefix='author')\ndf.fillna(0, inplace=True)\ndf['change_size'] = df['lines_added'] + df['lines_deleted']\ndf['add_delete_ratio'] = df['lines_added'] / (df['lines_deleted'] + 1)\nprint(\"Data preparation complete.\")\nprint(\"-\" * 40)\n# --- 4. Prepare Data and Split ---\nprint(\"\\n--- Step 4: Preparing and Splitting Data ---\")\ny = df['build_status']\nX = df.drop(columns=['pr_number', 'build_status'])",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "df['change_size']",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "df['change_size'] = df['lines_added'] + df['lines_deleted']\ndf['add_delete_ratio'] = df['lines_added'] / (df['lines_deleted'] + 1)\nprint(\"Data preparation complete.\")\nprint(\"-\" * 40)\n# --- 4. Prepare Data and Split ---\nprint(\"\\n--- Step 4: Preparing and Splitting Data ---\")\ny = df['build_status']\nX = df.drop(columns=['pr_number', 'build_status'])\nfeature_columns = X.columns.tolist()\njoblib.dump(feature_columns, 'feature_columns.pkl')",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "df['add_delete_ratio']",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "df['add_delete_ratio'] = df['lines_added'] / (df['lines_deleted'] + 1)\nprint(\"Data preparation complete.\")\nprint(\"-\" * 40)\n# --- 4. Prepare Data and Split ---\nprint(\"\\n--- Step 4: Preparing and Splitting Data ---\")\ny = df['build_status']\nX = df.drop(columns=['pr_number', 'build_status'])\nfeature_columns = X.columns.tolist()\njoblib.dump(feature_columns, 'feature_columns.pkl')\nX_train, X_test, y_train, y_test = train_test_split(",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "y",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "y = df['build_status']\nX = df.drop(columns=['pr_number', 'build_status'])\nfeature_columns = X.columns.tolist()\njoblib.dump(feature_columns, 'feature_columns.pkl')\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\nprint(\"-\" * 40)\n# --- 5. Apply SMOTE ---\nprint(\"\\n--- Step 5: Applying SMOTE to Balance Training Data ---\")",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "X",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "X = df.drop(columns=['pr_number', 'build_status'])\nfeature_columns = X.columns.tolist()\njoblib.dump(feature_columns, 'feature_columns.pkl')\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\nprint(\"-\" * 40)\n# --- 5. Apply SMOTE ---\nprint(\"\\n--- Step 5: Applying SMOTE to Balance Training Data ---\")\nsmote = SMOTE(random_state=42)",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "feature_columns",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "feature_columns = X.columns.tolist()\njoblib.dump(feature_columns, 'feature_columns.pkl')\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\nprint(\"-\" * 40)\n# --- 5. Apply SMOTE ---\nprint(\"\\n--- Step 5: Applying SMOTE to Balance Training Data ---\")\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "smote",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "smote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\nprint(\"SMOTE applied to training data.\")\nprint(\"-\" * 40)\n# --- 6. Hyperparameter Tuning for Random Forest (NEW STEP) ---\nprint(\"\\n--- Step 6: Hyperparameter Tuning for Random Forest ---\")\n# Define the grid of parameters to search through\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [10, 20, 30, None],",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "param_grid",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "param_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [10, 20, 30, None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'bootstrap': [True, False]\n}\n# We want to find the settings that give the best RECALL for the failure class.\nrecall_scorer = make_scorer(recall_score, pos_label=1)\n# Set up the randomized search. It will try 50 different combinations.",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "recall_scorer",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "recall_scorer = make_scorer(recall_score, pos_label=1)\n# Set up the randomized search. It will try 50 different combinations.\nrf = RandomForestClassifier(random_state=42)\nrf_random_search = RandomizedSearchCV(\n    estimator=rf,\n    param_distributions=param_grid,\n    n_iter=50,  # Number of combinations to try\n    cv=3,       # 3-fold cross-validation\n    verbose=1,\n    random_state=42,",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "rf",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "rf = RandomForestClassifier(random_state=42)\nrf_random_search = RandomizedSearchCV(\n    estimator=rf,\n    param_distributions=param_grid,\n    n_iter=50,  # Number of combinations to try\n    cv=3,       # 3-fold cross-validation\n    verbose=1,\n    random_state=42,\n    n_jobs=-1,  # Use all available CPU cores\n    scoring=recall_scorer # Optimize for recall!",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "rf_random_search",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "rf_random_search = RandomizedSearchCV(\n    estimator=rf,\n    param_distributions=param_grid,\n    n_iter=50,  # Number of combinations to try\n    cv=3,       # 3-fold cross-validation\n    verbose=1,\n    random_state=42,\n    n_jobs=-1,  # Use all available CPU cores\n    scoring=recall_scorer # Optimize for recall!\n)",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "tuned_rf",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "tuned_rf = rf_random_search.best_estimator_\nprint(\"-\" * 40)\n# --- 7. Train and Evaluate Final Models ---\nprint(\"\\n--- Step 7: Training and Evaluating Final Models ---\")\nscale_pos_weight = status_counts[0] / status_counts[1]\n# We now include our new, tuned Random Forest in the comparison\nmodels = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n    \"Tuned Random Forest\": tuned_rf, # Use the best one we found\n    \"XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "scale_pos_weight",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "scale_pos_weight = status_counts[0] / status_counts[1]\n# We now include our new, tuned Random Forest in the comparison\nmodels = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n    \"Tuned Random Forest\": tuned_rf, # Use the best one we found\n    \"XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n}\nbest_model = None\nbest_recall = -1\nfor name, model in models.items():",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "models",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "models = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n    \"Tuned Random Forest\": tuned_rf, # Use the best one we found\n    \"XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n}\nbest_model = None\nbest_recall = -1\nfor name, model in models.items():\n    print(f\"\\n--- Training {name} on SMOTE data ---\")\n    model.fit(X_train_resampled, y_train_resampled)",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "best_model",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "best_model = None\nbest_recall = -1\nfor name, model in models.items():\n    print(f\"\\n--- Training {name} on SMOTE data ---\")\n    model.fit(X_train_resampled, y_train_resampled)\n    y_pred = model.predict(X_test)\n    print(f\"Results for {name}:\")\n    report = classification_report(y_test, y_pred, output_dict=True)\n    print(classification_report(y_test, y_pred))\n    recall_failure = report.get('1', {}).get('recall', 0)",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "best_recall",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "best_recall = -1\nfor name, model in models.items():\n    print(f\"\\n--- Training {name} on SMOTE data ---\")\n    model.fit(X_train_resampled, y_train_resampled)\n    y_pred = model.predict(X_test)\n    print(f\"Results for {name}:\")\n    report = classification_report(y_test, y_pred, output_dict=True)\n    print(classification_report(y_test, y_pred))\n    recall_failure = report.get('1', {}).get('recall', 0)\n    if recall_failure > best_recall:",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "df = pd.read_csv('training_data.csv')\nprint(\"Dataset Info:\")\ndf.info()\nprint(\"\\nStatistical Summary:\")\nprint(df.describe())\n# Check the balance of our target variable ('build_status')\n# In most software projects, failures (1) are much rarer than successes (0).\n# This is called an \"imbalanced dataset.\"\nprint(\"\\nBuild Status Distribution:\")\nprint(df['build_status'].value_counts())",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "df = pd.get_dummies(df, columns=['author_association'], prefix='author')\n# For simplicity, we'll fill any potential missing values with 0.\ndf.fillna(0, inplace=True)\nprint(\"Data after one-hot encoding 'author_association':\")\nprint(df.head())\nprint(\"-\" * 40)\n# --- 3. Feature Engineering ---\nprint(\"\\n--- Step 3: Feature Engineering ---\")\n# Let's create some more intelligent features from the raw data.\n# A simple 'change_size' feature might be more predictive than additions/deletions alone.",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "df['change_size']",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "df['change_size'] = df['lines_added'] + df['lines_deleted']\n# The ratio of additions to deletions can indicate if a PR is a new feature vs. a refactor.\n# We add 1 to the denominator to avoid division by zero.\ndf['add_delete_ratio'] = df['lines_added'] / (df['lines_deleted'] + 1)\nprint(\"Data after adding new features ('change_size', 'add_delete_ratio'):\")\nprint(df[['pr_number', 'change_size', 'add_delete_ratio']].head())\nprint(\"-\" * 40)\n# --- 4. Model Training ---\nprint(\"\\n--- Step 4: Model Training ---\")\n# Define our target variable (what we want to predict)",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "df['add_delete_ratio']",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "df['add_delete_ratio'] = df['lines_added'] / (df['lines_deleted'] + 1)\nprint(\"Data after adding new features ('change_size', 'add_delete_ratio'):\")\nprint(df[['pr_number', 'change_size', 'add_delete_ratio']].head())\nprint(\"-\" * 40)\n# --- 4. Model Training ---\nprint(\"\\n--- Step 4: Model Training ---\")\n# Define our target variable (what we want to predict)\ny = df['build_status']\n# Define our features (the data we use to make the prediction)\n# We drop non-feature columns like the PR number and the original target.",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "y",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "y = df['build_status']\n# Define our features (the data we use to make the prediction)\n# We drop non-feature columns like the PR number and the original target.\nX = df.drop(columns=['pr_number', 'build_status'])\n# Save the feature column names. This is CRITICAL for our Flask app later.\n# It ensures the live data has the same columns in the same order as the training data.\nfeature_columns = X.columns.tolist()\njoblib.dump(feature_columns, 'feature_columns.pkl')\nprint(f\"Saved {len(feature_columns)} feature columns to feature_columns.pkl\")\n# Split the data into a training set (to teach the model) and a testing set (to evaluate it).",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "X",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "X = df.drop(columns=['pr_number', 'build_status'])\n# Save the feature column names. This is CRITICAL for our Flask app later.\n# It ensures the live data has the same columns in the same order as the training data.\nfeature_columns = X.columns.tolist()\njoblib.dump(feature_columns, 'feature_columns.pkl')\nprint(f\"Saved {len(feature_columns)} feature columns to feature_columns.pkl\")\n# Split the data into a training set (to teach the model) and a testing set (to evaluate it).\n# test_size=0.2 means we'll use 20% of the data for testing.\n# stratify=y is important for imbalanced datasets. It ensures the train and test sets\n# have the same proportion of failures and successes as the original dataset.",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "feature_columns",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "feature_columns = X.columns.tolist()\njoblib.dump(feature_columns, 'feature_columns.pkl')\nprint(f\"Saved {len(feature_columns)} feature columns to feature_columns.pkl\")\n# Split the data into a training set (to teach the model) and a testing set (to evaluate it).\n# test_size=0.2 means we'll use 20% of the data for testing.\n# stratify=y is important for imbalanced datasets. It ensures the train and test sets\n# have the same proportion of failures and successes as the original dataset.\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\nmodel.fit(X_train, y_train)\nprint(\"Model training complete.\")\nprint(\"-\" * 40)\n# --- 5. Model Evaluation ---\nprint(\"\\n--- Step 5: Model Evaluation ---\")\n# Make predictions on the unseen test data\ny_pred = model.predict(X_test)\n# Evaluate the model's performance\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")",
        "detail": "prediction-service.train_model",
        "documentation": {}
    },
    {
        "label": "y_pred",
        "kind": 5,
        "importPath": "prediction-service.train_model",
        "description": "prediction-service.train_model",
        "peekOfCode": "y_pred = model.predict(X_test)\n# Evaluate the model's performance\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\nprint(\"\\nClassification Report:\")\n# This report is the most important output. It tells us how the model\n# performs on the positive class (failures).\n# - Precision: Of all the PRs we predicted would fail, how many actually failed?\n# - Recall: Of all the PRs that actually failed, how many did we catch?\nprint(classification_report(y_test, y_pred))\nprint(\"-\" * 40)",
        "detail": "prediction-service.train_model",
        "documentation": {}
    }
]